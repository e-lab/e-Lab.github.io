<!DOCTYPE html>
<html lang="en-US">
<head>
   <meta charset="utf-8">
   <link rel="icon" type="image/ico" href="style/favicon.ico">
   <link rel="stylesheet" type="text/css" id="bootstrap.css" href="style/bootstrap.css">
   <title>e-Lab | Eugenio Culurciello's laboratory</title>

   <!-- include google analytics -->
   <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
   
      ga('create', 'UA-49283567-1', 'purdue.edu');
      ga('send', 'pageview');
   </script>

   <!-- include markdown webdocs -->
   <script src="document.min.js" userjs="user.js" icon="favicon.ico" id="DOC" onerror="var
      l=document.createElement('link'),s=document.createElement('script');l.rel='stylesheet';
      l.href='http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css';
      l.id='bootstrap.css';l.auto='1';document.head.insertBefore(l,document.getElementById('DOC'));
      s.src='http://aplib.github.io/document.min.js';document.head.appendChild(s);">
   </script>

   <!-- set page layout -->
   <!--[page-layout scheme=centered columns=100%]-->

</head>

<body>

<!-- display header & menu -->
<!--header-panel-->
<!--fixed-top-bar->header-bar-->
<!--header-bar-->


<!-- main contents begins here ----------------------> <!--content-panel

## Welcome to e-Lab!
---

**We want to give human-like vision to machines!**

We want computers to understand the environment around us, so that computers, phones, tablets, wearables, robots can be helpful in our daily activities.

We focus on [neuromorphic artificial vision systems]({{=$DOC.root}}html/research-intelligent-vision-systems.html).  We want to replicate human visual capabilities in a handheld device.  In other words, we want to design the eyes of robots – and with that possibly a large portion of their brain!

**Research questions:**
“How do we decompose images, and videos into a hierarchical set of base functions?”, “How do we use these functions to obtain the information content, meaning or just a compressed version of the original image?”, “What hardware can we use to obtan this decomposition in a short time?”: these are the scientific questions we are currently after.  Other questions can be found [here]({{=$DOC.root}}html/research-scientific-questions.html).

**Research philosophy:**
We specialize in the use of electrical technologies (microchips and systems of computing devices and sensors) to extend scientific exploratory methods and measurement tools for the endeavor of understanding life and replicating it in engineered systems.

Our research foundation is a multi-disciplinary environment encompassing: electrical engineering, biomedical sciences, computational neuroscience, machine learning, computer science, applied mathematics, biology, optics, physics, electrochemistry, to name a few.

Find us on [GitHub](https://github.com/organizations/e-lab). 

Share us on [Twitter](https://twitter.com/share), [Facebook](http://www.facebook.com/sharer/sharer.php?u=http://engineering.purdue.edu/elab/") and [Google+](http://plus.google.com/share?url={http://engineering.purdue.edu/elab}).


---
## News

**March 2014:** Our nn-X vision system has been featured on the [Journal and Courier](http://www.jconline.com/videonetwork/3399982126001/Purdue-research-allows-cell-phone-to-behave-like-human-brain), on [18WLFI TV](http://wlfi.com/2014/03/19/purdue-researchers-working-to-make-smartphones-smarter/), [Business Insider](http://www.businessinsider.com.au/human-like-tech-for-phones-2014-3), [Phys org](http://phys.org/news/2014-03-enabling-smartphones.html), [the Engineer](http://www.theengineer.co.uk/electronics/news/deep-learning-gives-visual-identity-to-smartphones/1018223.article),  [Imperial Valley News](http://www.imperialvalleynews.com/index.php/news/latest-news/7668-smartphone-to-become-smarter-with-deep-learning-innovation.html), [Purdue News](http://www.purdue.edu/newsroom/releases/2014/Q1/smartphone-to-become-smarter-with-deep-learning-innovation.html), [Phone Arena](http://www.phonearena.com/news/Smartphones-will-get-smarter-with-deep-learning-developments_id54151), [Hoosier AG Today](http://www.hoosieragtoday.com/smartphone-to-become-smarter-with-deep-learning-innovation/), [CN beta](http://www.cnbeta.com/articles/277401.htm), to name a few!

<img src={{=$DOC.root}}data/img/JC-3-2014-ec-2.png width=100% style="max-width:450px;">

**February 2014:** We have been invited to the Embedded Vision Workshop at CVPR 2014, to present our new [vision system nn-X](http://www.youtube.com/watch?v=7I-zLjLKZV8).

**January 2014:** Our nn-X vision system, presented at NIPS 2013, has been featured on the [MIT Technology Review](http://www.technologyreview.com/news/523181/an-ai-chip-to-help-computers-understand-images/), on [BBC](http://www.bbc.co.uk/programmes/p01pfmzl), [FierceWireless](http://www.fiercewireless.com/tech/story/teradeep-hopes-see-apple-qualcomm-others-use-its-artificial-intelligence-te/2014-01-05), and more!

October 2013: Our neural network micro-chips find a use by cell-phone giants, such as Qualcomm: see [EmTech MIT videos](http://www2.technologyreview.com/emtech/13/video/day1/) (on right: select talk by Matt Grob), and also read this [report](http://m.technologyreview.com/news/520211/qualcomm-to-build-neuro-inspired-chips/).

**September 2013:** Our self-published book is finally out: “Biomedical Circuits and Systems”, Integrated Instrumentation, by Wei Tang, Evan Joon Hyuk Park, Brian Goldstein, Dongsoo Kim, Pujitha Weerakoon, Eugenio Culurciello, e-Lab. Lulu 2013. [Ordering info](http://www.lulu.com/shop/eugenio-culurciello-and-wei-tang-and-evan-joon-hyuk-park-and-brian-goldstein/biomedical-circuits-and-systems/hardcover/product-21233739.html#productDetails).

**March 2013:** our [first open access](http://arxiv.org/abs/1301.2820) paper got accepted at a new revolutionary conference with open acces and open review systems. Please look at [here](http://openreview.net/) and [there](https://sites.google.com/site/representationlearning2013/)

**September 2012:** neuFlow SoC (sytem-on-a-chip) arrived from the IBM 45 nm factory!

<img src={{=$DOC.root}}data/img/neuflow-soc.png width=100% style="max-width:548px;">

**July 2012:** Our work on hardware accelerated visual system is featured on [Popular Science](http://www.popsci.com/technology/article/2012-07/how-teach-robot-improvise).

**February 2012:** Eugenio Culurciello gave a talk at the Yahoo-sponsored Machine Learning Seminar at Purdue: Talk Slides.

**February 2012:** Eugenio Culurciello Purdue BME seminar Feb 15th 2012: "Scaling up neuroscience: optogenetic neural recording": [Talk Slides](http://www.youtube.com/watch?v=iGcVhkkDSho&list=UUQQzotkfVnXzBUMzpPaVdNw&index=1&feature=plcp).

**November 2011:** Eugenio Culurciello gave a keynote address at the at [System-On-A-Chip conference 2011](http://www.socconference.com/). A video is available [here](http://www.youtube.com/watch?v=bgtX3JYdUTQ).

**July 2011:** Eugenio Culurciello is now an Associate Professor at Purdue University Weldon School of Biomedical Engineering. e-Lab has now moved to Purdue!

**June 2011:** Eugenio Culurciello became a Senior Member of the IEEE.

**May 2011:** We have presented 3 papers IEEE ISCAS/CASFEST 2011 in Rio, Brazil. For details please refer to publications.

**April 2011:** Eugenio Culurciello and e-Lab is moving to Purdue University in July.

**November 2010:** Eugenio Culurciello was selected as recipient of the Presidential Early Career Award for Scientists and Engineers (PECASE). The PECASE is the highest honor bestowed by the United States government on outstanding scientists and engineers in the early stages of their independent research careers. A photo of the event:

<img src=data/img/pecase.jpg width=100%>

**November 2010:** Eugenio Culurciello was elected to be a Distinguished Lecturer of the IEEE by the society of Circuits and Systems (CASS) for 2011-2012 by the Neural Systems & Applications Technical Committee. Distinguished Lecturer of the IEEE are engineering professionals who help lead their fields in new technical developments that shape the global community. As a Distinguished Lecturer, IEEE will support travel to selected institutions to present our research work and results, and also publicize our school and activities.

**September 2010:** BREAKTHROUGH: NeuFlow, our first synthetic model of the mammalian visual system in hardware is live!. This breakthrough was featured in the media ([MSNBC](http://www.msnbc.msn.com/id/39233118), [New York Times podcast](http://dts.podtrac.nytimes.com/redirect.mp3/podcasts.nytimes.com/podcasts/2010/10/06/07techtalk.mp3), [the Economist](http://www.economist.com/node/17305316), [International Business Times](http://www.ibtimes.com/articles/63070/20100916/supercomputers-microchips-self-driving-cars.htm), [Science Daily](http://www.sciencedaily.com/releases/2010/09/100915171544.htm), [gizmag](http://www.gizmag.com/neuflow-seeing-supercomputer/16387/), [Yale News](http://opa.yale.edu/news/article.aspx?id=7752), to name a few – Google “Neuflow” or “NeoVision + Culurciello” to get them all) in the Fall 2010.

**July 2010:** BREAKTHROUGH: Our Voltage-Sensitive Dye Imaging (VSDI) system was tensted on the mouse visual cortex at the University of Osaka, with prof. Tetsuya Yagi. A summary of results will follow.

**June 2010:** BREAKTHROUGH: Our patch-clamp system [was published](https://engineering.purdue.edu/elab/blog/wp-content/uploads/2011/12/JNM2010.pdf) in the Journal of Neuroscience Methods. A grant from NIH (phase II STTR with Warner Instrument) was set up to commecialize this device.

**March 2010:** Eugenio Culurciello and his team won a DARPA award to develop hardware for synthetic vision in the context of project NeoVision2.

**February 2010:** Eugenio Culurciello and his team won an ONR MURI award titled: “Figure-Ground Processing, Saliency and Guided Attention for Analysis of Large Natural Scenes”.

**January 2010:** Six of our papers were accepted for presentation at IEEE ISCAS 2010 in Paris.

**August 2009:** The book “Silicon-on-Sapphire Circuits and Systems, Sensor and Biosensor interfaces by Eugenio Culurciello has been published by McGraw Hill and is available [here](http://www.mhprofessional.com/product.php?isbn=0071608486).

**July 2009:** Eugenio Culurciello won an NSF award for the project: “A lightweight event-based synthetic vision system for assisted-living and machine vision applications”.

**June 2009:** Eugenio Culurciello and Vincent Pieribone won an NIH award for the project: “High-Speed, Wide Field Fluorescent Imaging of Cortex in Freely Moving Animals (R01 NS065110-01)”.

**May 2009:** We have presented 4 papers and two demos at IEEE ISCAS 2009 in Taipei, Taiwan. For details please refer to publications.

**April 2009:** Eugenio Culurciello was promoted to Associate Professor of Electrical Engineering.

**November 2008:** Eugenio Culurciello was invited to the National Academies Keck Foundation Future Initiatives Conference, where he was part of group 6B on the study of the human brain.

**October 2008:** We presented an invited paper at the 2008 IEEE International SOI conference titled: “Mixed Signal Microsystems in Emerging SOI Technologies”.

**May 2008:** We obtained ONR funding for our project on optical brain-machine interfaces, the title of the project is “A high-speed, in-vivo optical neural recording system”.

**May 2008:** Our paper “Fall Detection Using an Address-Event Temporal Contrast Vision Sensor”, has won the Best Paper Award at the 2008 IEEE International Symposium on Circuits and Systems, Seattle USA, May 18-21, 2008.

**January 2008:** We obtained ONR funding for our project on hyperbaric instruments for electro-physiology, the title of the project is “Integrated Microscale Biosensor for Cell Membrane and Patch-Clamp”.

**January 2008:** Eugenio Culurciello is writing a book on SOS circuits and systems with the Peregrine Semiconductors Process.

**January 2008:** All six of our submitted papers were accepted for oral presentation at IEEE ISCAS 2008. For details please refer to publications.

**September 2007:** We partnered with Peregrine Semiconductors to evaluate ultra low-power sensory and communication interfaces.

**August 2007:** We obtained ARO seed funding for our project on voltage-sensitive-dyes-imaging titled: “Development of an Implantable Optical Neuroprosthetic: System Integration and Testing” with Vincent Pieribone (Yale-Pierce Fnd.).

**July 2007:** We obtained NSF funding for our project on high-density patch-clamp amplifiers titled: “IDBR: High-Performance Integrated Patch Clamp Amplifiers” with prof. Fred Sigworth (Yale).

**January 2007:** 5 papers were accepted at IEEE ISCAS 2007. For details please refer to publications.

**October 2006:** We participated to the 2nd 3D-run by MIT Lincoln Laboratories with the design of a Silicon Retina.

**September 2006:** We obtained NSF funding for our project titled: “A Lightweight Event-Driven Network of Biomemetic Image Sensors” with A. Savvides (Yale).

**March 2006:** We co-developed an AER image sensor simulator to test the sensors before fabrication using a webcam. For details please look [here](http://www.eng.yale.edu/enalab/AERnets.html).

**March 2005:** Farah won the best student paper award for: “Integrated Patch clamp amplifier”, Farah Laiwalla, Zhengming Fu, Kate Klemic, Fred Sigworth, Eugenio Culurciello, Yale University CMOC Symposium March 17th, 2005.


--> <!-- main contents ends here -------------------------------------->

<!--footer-panel-->

<noscript><div class="alert alert-warning">This Website requires your browser to be JavaScript enabled.</div></noscript>
</body>
</html>
