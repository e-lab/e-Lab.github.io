<!DOCTYPE html>
<html lang="en-US">
<head>
   <meta charset="utf-8">
   <link rel="icon" type="image/ico" href="../style/favicon.ico">
   <link rel="stylesheet" type="text/css" id="bootstrap.css" href="../style/bootstrap.css">
   <title>e-Lab | Eugenio Culurciello's laboratory</title>

   <!-- include google analytics -->
   <script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
   
      ga('create', 'UA-49283567-1', 'purdue.edu');
      ga('send', 'pageview');
   </script>

   <!-- include markdown webdocs -->
   <script src="../document.min.js" userjs="user.js" icon="favicon.ico" id="DOC" onerror="var
      l=document.createElement('link'),s=document.createElement('script');l.rel='stylesheet';
      l.href='http://netdna.bootstrapcdn.com/bootstrap/3.0.3/css/bootstrap.min.css';
      l.id='bootstrap.css';l.auto='1';document.head.insertBefore(l,document.getElementById('DOC'));
      s.src='http://aplib.github.io/document.min.js';document.head.appendChild(s);">
   </script>

   <!-- set page layout -->
   <!--[page-layout scheme=centered columns=100%]-->

</head>

<body>

<!-- display header & menu -->
<!--header-panel-->
<!--fixed-top-bar->header-bar-->
<!--header-bar-->


<!-- main contents begins here ----------------------> <!--content-panel


## BME 495A – Homework

### Computational Neuroscience and Learning: basic

BME 495A, Spring 2014


#### This class has an online and and an interactive component!

Students are required to watch the video lectures posted online, they can ask question in class about any lecture. Please help us make these course better: post your questions as comments to the youtube video lectures. You can also link to other material and other lectures.


#### Homework instructions:

Submit your homework based on schedule below. Submit a small 1-4 pages report together with your code and files. Please use PDF for the report.
Report: write a formal report with these sections: intro, methods, results, discussion.
Code: include your code and its output in your report and homework zip file
Code results: include code results in report, show the code output or record a code output video if necessary.
Also do not forget the essential information:

+ write your name, HW number and date
+ send files with name: “firstname-HW#-BME495A.xxx”
+ include all details of your work, implementation, algorithms, diagrams, figures…



#### Week 15: Monday May 5th:

+ Final project reports due by 12 noon. Each students shuould submit a max 4-page report on the project, listing: Introduction, methods, results, discussion. 


#### Week 14: April 29th and May 1st:

+ Final project presentations. Each group presents for 10 minutes, including setup time. Please present introduction, methods and results. Prepapre a demonstation video for your final presentation. Submit a pdf copy of your presentationa nd l9nk to videos/extra material on the same day you present.


#### Week 13: April 22nd

+ Project reports:
+ Hardware for deep neural networks

#### Week 12: April 15th

+ Project reports:
+ Tracking with deep neural networks

#### Week 11: April 8th

+ Project reports:
+ Unsupervised deep neural networks


#### Week 10: April 1st

+ Project reports:
+ multiscale analysis
+ random networks


#### Week 9: March 25th

+ unsupervised learning
+ [video lectures](http://www.youtube.com/playlist?list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL&feature=view_all) – lecture 6 and 7, all modules
+ slides: [6-unsup]({{=$DOC.root}}data/doc/2011-12-6-unsup.pdf) [7-CL-lecture]({{=$DOC.root}}data/doc/2011-12-7-CL-lecture.pdf)
+ Project presentations: each students should present 5 minutes presentation (with slides, supporting material, anything  they need). Please load your presentation on the class computer or bring your own laptop. You have 1 minute of setup time. Please test before class to avoid delays!


#### Week 8: March 18th

+ Spring Break

 
#### Week 7: March 11st

+ Code in Torch for Convolutional neural networks
+ [video lectures](https://www.youtube.com/playlist?feature=edit_ok&list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL) - lecture 4 and 5, all modules
+ Train-a-face-detector [example here](https://github.com/e-lab/torch7-demos/tree/master/train-a-face-detector-new)
+ Running demo [example here](https://github.com/e-lab/torch7-demos/tree/master/face-detector-new)
+ [Homework 4 due March 13th, part II of homework below](https://github.com/clementfarabet/torch7-demos).
 

#### Week 6: March 4th

+ Tue: review of homework 3: results and issues
+ Train-a-face-detector [example here](https://github.com/e-lab/torch7-demos/tree/master/train-a-face-detector-new)
+ Running demo [example here](https://github.com/e-lab/torch7-demos/tree/master/face-detector-new)
+ Homework 4 due March 6th – part I
  - Using Torch and our extended neural network package, and based on the train-a-face-detector [example here](https://github.com/e-lab/torch7-demos/tree/master/train-a-face-detector-new), train a net to recognize faces and hands in an image.
  - Part 1: hands, part II: faces+hands.
  - Create your own datasets for hands: record your hand moving around the room constantly and changing hand position. Make sure the hand covers ~80% of the image. Generate about as many samples as for the example face detector. You can record ~3600 frames for a minute of movie, and you can use a script to generate 10 extra frames for each frame of video by rotating the image by +/- 10deg and flipping horizontally. Or you can just record more video… Use “ffmpeg” [to convert the video from frames](http://linuxers.org/tutorial/how-extract-images-video-using-ffmpeg):

        require 'ffmpeg'
        dspath = '../video_file' -- pointer to file path
        source = ffmpeg.Video{path=dspath, encoding='jpg',loaddump=true, load=false} -- load video frames from source file
        rawFrame = source:forward() -- grab next frame
    
  - Start with getting the dataset and then training, then use the trained network in a running demonstration.


#### Week 5: Feb 18th

+ Convolutional neural networks
+ convolution operation, 2D representation of images, local features, multi-layer convolutional networks
+ [video lectures](https://www.youtube.com/playlist?feature=edit_ok&list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL) - lecture 3, all modules
+ [slides]({{=$DOC.root}}data/doc/2011-12-3-convnets.pdf)
+ Homework 3: due Feb 20th
  - Use Torch Neural Network packages to repeat HW2 part A, B. See [here](https://github.com/torch/nn/) (see: Training a neural network) for a good example of how to do this.


#### Week 4: Feb 11th

+ we discuss coding styles and results during the training of neural networks
+ we also discuss 2D neural networks and convolution operations and how they should be used in processing images
+ [video lectures](https://www.youtube.com/playlist?feature=edit_ok&list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL) - lecture 2, all modules
+ [slides]({{=$DOC.root}}data/doc/2011-12-2-neural_nets1.pdf), and [slides]({{=$DOC.root}}data/doc/2014-01-back-propagation.pdf) from Jonghoon 20140120-back-propagation
+ [Torch/Lua tutorial]({{=$DOC.root}}data/doc/2014-02-lua-tutorial.txt) from Ayse
+ Homework 2: due Feb 13th for part B


#### Week 3: Feb 4th

+ we continue to study the backprop algorithm for training neural networks
+ [video lectures](https://www.youtube.com/playlist?feature=edit_ok&list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL) - lecture 2, all modules – focus on backprop!
+ [slides]({{=$DOC.root}}data/doc/2011-12-2-neural_nets1.pdf), and [slides]({{=$DOC.root}}data/doc/2014-01-back-propagation.pdf) from Jonghoon 20140120-back-propagation
+ Homework 2: due Feb6th for part A


#### Week 2: Jan 28th

+ Introduction to neural networks
+ [video lectures](https://www.youtube.com/playlist?feature=edit_ok&list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL) - lecture 2, all modules
+ [slides]({{=$DOC.root}}data/doc/2011-12-2-neural_nets1.pdf)
+ Homework 2: due Feb6th for part A, Feb 13th for part B


**Part A:** Write a simple with low-level (C or else) code of a neural network. Write code that can take two input and solve the X(N)OR problem. Create a XNOR dataset {x1,x2,y} = {{1,1,1}, {0,1,0},…}, then train a simple 2-layer network, as in: [Perceptrons](http://www.generation5.org/content/1999/perceptron.asp) and [Back-propagation](http://www.generation5.org/content/2002/bp.asp) ([BP Example: XOR Net](http://www.generation5.org/content/2001/xornet.asp), [Back-Propagation: CBPNet](http://www.generation5.org/content/2000/cbpnet.asp) and another [XOR Matlab code](http://www.philbrierley.com/main.html?code/matlab.html&code/codeleft.htm)).

**Part B:** Train a [Multi Layer Perceptron (MLP)](http://en.wikipedia.org/wiki/Multilayer_perceptron) to recognize a 5×5 image:
Create a binary 5×5 image of a symbol or two and train a large MLP with as many inputs as pixels to recognize the symbol. Try to test on a few “noisy” symbols by switching 1,2,3… pixels and reporting performance of recognition as a function of “noise” = bits switched.

**More specifically:**

+ create a binary symbol (5×5 + sign) that is positive the object to look for
+ create a dataset by mixing positive symbols (dirtied with n pixels of random noise) with examples of random patterns
+ learn weights of network with back-propagation
+ test on a dataset of symbols currupted by noise:
  - add 1,2,3,… up to 5×5/2 (half pixel noise)
  - plot test set precision vs noise
+ Paper to read: [SerrePoggioACM10]({{=$DOC.root}}data/doc/2011-12-SerrePoggioACM10.pdf)


#### Week 1: Jan 14th

+ Introduction to class and neural networks
  - [video lectures](https://www.youtube.com/playlist?feature=edit_ok&list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL) - lecture 1, all modules, including Computational Neuroscience lectures (3 videos)
  - [slides]({{=$DOC.root}}data/doc/2011-12-1-intro.pdf), [slides CompNeuro]({{=$DOC.root}}data/doc/1022-12-1.1-brain_vision.key.pdf)
+ Homework 1: Install torch, torch tutorials “1_getstarted” – due Jan 23rd
  - https://github.com/clementfarabet/torchinstall
  - https://github.com/andresy/torch
+ Getting Started with Torch
  - http://code.cogbits.com/wiki/doku.php?id=tutorial_basics
  - [Torch 7 – Setting the environment up.pptx]({{=$DOC.root}}data/doc/2011-12-Torch-7-Setting-the-environment-up.pptx.pdf)
  - [Torch7-biglearn2011]({{=$DOC.root}}data/doc/2011-12-Torch7-biglearn20111.pdf)
+ paper to read: [Torch7-biglearn2011]({{=$DOC.root}}data/doc/2011-12-Torch7-biglearn20111.pdf)
 

---
### OLD 2013 SCHEDULE (not valid for this year until pushed above):

#### Week 15: April 15th

+ work on project
+ project final presentations


#### Week 14: April 8th

+ work on project
+ the neuFlow hardware visual system


#### Week 13: April 1st

+ work on project
+ visual attention


#### Week 11: March 25th

+ work on project
+ scene parsing with deep networks


#### Week 11: March 18th

+ project proposals presentations (Tue)


#### Week 10: March 11th

+ project proposals presentations
+ paper [Clement PAMI](http://yann.lecun.com/exdb/publis/pdf/farabet-pami-13.pdf), presented by Nathan


#### Week 9: March 4th

+ online learning with deep networks
+ [video lectures](http://www.youtube.com/playlist?list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL&feature=view_all) - lecture 10, all modules
+ slides: [here]({{=$DOC.root}}data/doc/2011-12-online-learner1.pdf)
+ Thu paper: [coatesng_nntot2012]({{=$DOC.root}}data/doc/2011-12-coatesng_nntot2012.pdf) presented by Alfredo
+ homework: prepare project proposal


#### Week 8: Mar 4th

+ vision systems with deep networks
+ [video lectures](http://www.youtube.com/playlist?list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL&feature=view_all) - lecture 9, all modules
+ slides: [here]({{=$DOC.root}}data/doc/2011-12-Lecture9.pdf)
+ Homework 6 due 2/28: train a 2-layer deep network with Clustering Learning as a face detector. Use dataset from previous homework. Use CL code provided in lectures.
+ Thu [paper](http://arxiv.org/abs/1301.2820), presented by Cosmo Zhang


#### Week 7: Feb 25th

+ more about Deep Networks: connections and pooling, etc.
+ [video lectures](http://www.youtube.com/playlist?list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL&feature=view_all) - lecture 8, all modules
+ slides: [8-moreonDeepNets]({{=$DOC.root}}data/doc/2011-12-8-moreonDeepNets.pdf) and [this one]({{=$DOC.root}}data/doc/2011-12-Lecture8+.pdf)
+ Homework: extra time for previous homework 4, 5
+ Thu: attend seminar: Interactive Machine Learning with Humans in the Loop
+ Dr. Yisong Yue
  - Carnegie Mellon University, Computer Science
  - Thursday, February 21, 2013, 10:30 am, LWSN 3102 A/B
 

#### Week 6: Feb 18th

+ unsupervised learning
+ [video lectures](http://www.youtube.com/playlist?list=PLNgy4gid0G9e0-SiJWEdNEcHUYjo1T5XL&feature=view_all) – lecture 6 and 7, all modules
+ slides: [6-unsup]({{=$DOC.root}}data/doc/2011-12-6-unsup.pdf) [7-CL-lecture]({{=$DOC.root}}data/doc/2011-12-7-CL-lecture.pdf)
+ autencoders, sparse coding etc tutorial code: http://code.cogbits.com/wiki/doku.php?id=tutorial_unsupervised
+ Clustering Learning [code](https://github.com/culurciello/CL_paper1_code)
+ Thu paper: [mathpsych-tutorial]({{=$DOC.root}}data/doc/2011-12-mathpsych-tutorial1.pdf) presented by Nathan
+ Homework 5 due Thu 2/21: clustering learning techniques to learn base functions (encoder) of this dataset: http://data.neuflow.org/data/faces_cut_yuv_32x32.tar.gz, then create the decoder code that takes an image from the dataset and reconstructs it using the base-functions of the encoder. Show reconstructed image as a function of base function used (4, 8, 16, etc) and also compute the MSE each time.


--> <!-- main contents ends here -------------------------------------->

<!--footer-panel-->

<noscript><div class="alert alert-warning">This Website requires your browser to be JavaScript enabled.</div></noscript>
</body>
</html>
